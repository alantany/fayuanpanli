#!/usr/bin/env python3
"""
æ³•é™¢åˆ¤ä¾‹çŸ¥è¯†åº“ - ä¼˜åŒ–çš„æ•°æ®åº“æ¢å¤è„šæœ¬
é¿å…åœ¨äº‘ç«¯ä¸‹è½½embeddingæ¨¡å‹ï¼Œä½¿ç”¨é¢„è®¡ç®—çš„å‘é‡
"""

import os
import sys
import shutil
import json
import chromadb
import logging
from pathlib import Path

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def clean_and_recreate_db():
    """å®Œå…¨æ¸…ç†å¹¶é‡æ–°åˆ›å»ºæ•°æ®åº“ç›®å½•"""
    db_path = "db/"
    
    logger.info("ğŸ§¹ å®Œå…¨æ¸…ç†æ•°æ®åº“ç›®å½•...")
    
    # å®Œå…¨åˆ é™¤æ•°æ®åº“ç›®å½•
    if os.path.exists(db_path):
        shutil.rmtree(db_path)
        logger.info("âœ… å·²åˆ é™¤æ—§æ•°æ®åº“ç›®å½•")
    
    # é‡æ–°åˆ›å»ºç›®å½•
    os.makedirs(db_path, exist_ok=True)
    logger.info("âœ… å·²åˆ›å»ºæ–°æ•°æ®åº“ç›®å½•")
    
    # è®¾ç½®æ­£ç¡®çš„æƒé™
    os.chmod(db_path, 0o755)
    logger.info("âœ… å·²è®¾ç½®ç›®å½•æƒé™")

def import_data_with_vectors():
    """ä»JSONæ ¼å¼å¯¼å…¥æ•°æ®ï¼ŒåŒ…å«é¢„è®¡ç®—çš„å‘é‡"""
    # ä¼˜å…ˆä½¿ç”¨åŒ…å«å‘é‡çš„JSONæ–‡ä»¶
    json_files = ["db_export_with_vectors.json", "db_export.json"]
    json_file = None
    
    logger.info("ğŸ” æ£€æŸ¥å¯ç”¨çš„JSONæ–‡ä»¶...")
    for file in json_files:
        if os.path.exists(file):
            file_size = os.path.getsize(file) / 1024 / 1024
            logger.info(f"   ğŸ“ æ‰¾åˆ°æ–‡ä»¶: {file} ({file_size:.1f} MB)")
            if json_file is None:  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„æ–‡ä»¶ï¼ˆä¼˜å…ˆçº§é¡ºåºï¼‰
                json_file = file
                logger.info(f"   âœ… é€‰æ‹©ä½¿ç”¨: {file}")
    
    if not json_file:
        logger.error(f"âŒ æœªæ‰¾åˆ°JSONå¯¼å‡ºæ–‡ä»¶")
        logger.info("è¯·ç¡®ä¿ä»¥ä¸‹æ–‡ä»¶ä¹‹ä¸€å­˜åœ¨:")
        for file in json_files:
            logger.info(f"   - {file}")
        return False
    
    logger.info(f"ğŸ“¥ ä»JSONå¯¼å…¥æ•°æ®: {json_file}")
    
    try:
        logger.info("ğŸ“– è¯»å–JSONæ–‡ä»¶...")
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # æ˜¾ç¤ºå¯¼å…¥ä¿¡æ¯
        metadata = data.get("_metadata", {})
        includes_vectors = metadata.get("includes_vectors", False)
        
        logger.info(f"ğŸ“Š å¯¼å…¥ä¿¡æ¯:")
        logger.info(f"   - æ–‡ä»¶å: {json_file}")
        logger.info(f"   - æºå¹³å°: {metadata.get('source_platform', 'unknown')}")
        logger.info(f"   - å¯¼å‡ºæ—¶é—´: {metadata.get('export_time', 'unknown')}")
        logger.info(f"   - æ€»æ–‡æ¡£æ•°: {metadata.get('total_documents', 'unknown')}")
        logger.info(f"   - åŒ…å«å‘é‡: {'âœ…' if includes_vectors else 'âŒ'}")
        
        if includes_vectors:
            logger.info("ğŸ¯ æ£€æµ‹åˆ°é¢„è®¡ç®—å‘é‡ï¼Œå°†é¿å…æ¨¡å‹ä¸‹è½½")
        else:
            logger.warning("âš ï¸  æœªæ£€æµ‹åˆ°å‘é‡æ•°æ®ï¼Œå¯èƒ½ä¼šè§¦å‘æ¨¡å‹ä¸‹è½½")
        
        # åˆå§‹åŒ–ChromaDBå®¢æˆ·ç«¯
        logger.info("ğŸ”— åˆå§‹åŒ–ChromaDBå®¢æˆ·ç«¯...")
        client = chromadb.PersistentClient(path="db/")
        total_imported = 0
        
        for collection_name, collection_data in data.items():
            # è·³è¿‡å…ƒæ•°æ®
            if collection_name == "_metadata":
                continue
                
            logger.info(f"ğŸ“¥ å¯¼å…¥é›†åˆ: {collection_name}")
            
            try:
                # åˆ›å»ºé›†åˆ
                collection = client.get_or_create_collection(name=collection_name)
                
                # æ£€æŸ¥é›†åˆæ•°æ®ç»“æ„
                if not isinstance(collection_data, dict):
                    logger.error(f"âŒ é›†åˆæ•°æ®æ ¼å¼é”™è¯¯: {collection_name}")
                    continue
                
                documents = collection_data.get('documents', [])
                metadatas = collection_data.get('metadatas', [])
                ids = collection_data.get('ids', [])
                embeddings = collection_data.get('embeddings', [])
                
                logger.info(f"   ğŸ“Š é›†åˆæ•°æ®ç»Ÿè®¡:")
                logger.info(f"      - æ–‡æ¡£æ•°: {len(documents)}")
                logger.info(f"      - å…ƒæ•°æ®æ•°: {len(metadatas)}")
                logger.info(f"      - IDæ•°: {len(ids)}")
                logger.info(f"      - å‘é‡æ•°: {len(embeddings)}")
                
                if documents and len(documents) > 0:
                    # ç¡®ä¿æ‰€æœ‰æ•°æ®æ•°é‡åŒ¹é…
                    if not metadatas:
                        metadatas = [{}] * len(documents)
                    if not ids:
                        ids = [f"doc_{i}" for i in range(len(documents))]
                    
                    # å¦‚æœæœ‰é¢„è®¡ç®—çš„å‘é‡ï¼Œä½¿ç”¨å®ƒä»¬ï¼›å¦åˆ™è®©ChromaDBè®¡ç®—
                    if embeddings and len(embeddings) == len(documents):
                        logger.info(f"   ğŸ¯ ä½¿ç”¨é¢„è®¡ç®—å‘é‡ï¼Œé¿å…æ¨¡å‹ä¸‹è½½")
                        # æ‰¹é‡å¯¼å…¥æ•°æ®ï¼ˆåŒ…å«å‘é‡ï¼‰
                        batch_size = 100  # å¢åŠ æ‰¹æ¬¡å¤§å°ï¼Œå› ä¸ºä¸éœ€è¦è®¡ç®—å‘é‡
                        for i in range(0, len(documents), batch_size):
                            batch_docs = documents[i:i+batch_size]
                            batch_metas = metadatas[i:i+batch_size]
                            batch_ids = ids[i:i+batch_size]
                            batch_embeddings = embeddings[i:i+batch_size]
                            
                            # ç¡®ä¿æ‰¹æ¬¡æ•°æ®é•¿åº¦ä¸€è‡´
                            min_len = min(len(batch_docs), len(batch_metas), len(batch_ids), len(batch_embeddings))
                            batch_docs = batch_docs[:min_len]
                            batch_metas = batch_metas[:min_len]
                            batch_ids = batch_ids[:min_len]
                            batch_embeddings = batch_embeddings[:min_len]
                            
                            collection.add(
                                documents=batch_docs,
                                metadatas=batch_metas,
                                ids=batch_ids,
                                embeddings=batch_embeddings
                            )
                            
                            logger.info(f"   âœ… å¯¼å…¥æ‰¹æ¬¡ {i//batch_size + 1}: {len(batch_docs)} ä¸ªæ–‡æ¡£ï¼ˆå«å‘é‡ï¼‰")
                    else:
                        logger.warning(f"   âš ï¸  æ²¡æœ‰é¢„è®¡ç®—å‘é‡ï¼Œå°†è§¦å‘æ¨¡å‹ä¸‹è½½")
                        # æ‰¹é‡å¯¼å…¥æ•°æ®ï¼ˆä¸å«å‘é‡ï¼Œä¼šè§¦å‘embeddingè®¡ç®—ï¼‰
                        batch_size = 20  # å‡å°æ‰¹æ¬¡å¤§å°ï¼Œé¿å…å†…å­˜é—®é¢˜
                        for i in range(0, len(documents), batch_size):
                            batch_docs = documents[i:i+batch_size]
                            batch_metas = metadatas[i:i+batch_size]
                            batch_ids = ids[i:i+batch_size]
                            
                            # ç¡®ä¿æ‰¹æ¬¡æ•°æ®é•¿åº¦ä¸€è‡´
                            min_len = min(len(batch_docs), len(batch_metas), len(batch_ids))
                            batch_docs = batch_docs[:min_len]
                            batch_metas = batch_metas[:min_len]
                            batch_ids = batch_ids[:min_len]
                            
                            collection.add(
                                documents=batch_docs,
                                metadatas=batch_metas,
                                ids=batch_ids
                            )
                            
                            logger.info(f"   âœ… å¯¼å…¥æ‰¹æ¬¡ {i//batch_size + 1}: {len(batch_docs)} ä¸ªæ–‡æ¡£")
                    
                    total_imported += len(documents)
                    logger.info(f"âœ… å®Œæˆå¯¼å…¥ {len(documents)} ä¸ªæ–‡æ¡£åˆ° {collection_name}")
                else:
                    logger.warning(f"âš ï¸  é›†åˆ {collection_name} æ²¡æœ‰æ–‡æ¡£æ•°æ®")
                
            except Exception as e:
                logger.error(f"âŒ å¯¼å…¥é›†åˆå¤±è´¥ {collection_name}: {e}")
                continue
        
        logger.info(f"âœ… JSONæ•°æ®å¯¼å…¥å®Œæˆï¼Œæ€»è®¡å¯¼å…¥ {total_imported} ä¸ªæ–‡æ¡£")
        return True
        
    except Exception as e:
        logger.error(f"âŒ JSONå¯¼å…¥å¤±è´¥: {e}")
        return False

def verify_database():
    """éªŒè¯æ•°æ®åº“çŠ¶æ€"""
    logger.info("ğŸ” éªŒè¯æ•°æ®åº“çŠ¶æ€...")
    
    try:
        client = chromadb.PersistentClient(path="db/")
        collections = client.list_collections()
        
        if len(collections) == 0:
            logger.error("âŒ æ•°æ®åº“ä¸ºç©º")
            return False
        
        total_docs = 0
        for collection_info in collections:
            collection = client.get_collection(collection_info.name)
            count = collection.count()
            total_docs += count
            logger.info(f"   ğŸ“ {collection_info.name}: {count} ä¸ªæ–‡æ¡£")
        
        logger.info(f"âœ… éªŒè¯æˆåŠŸ: {len(collections)} ä¸ªé›†åˆï¼Œ{total_docs} ä¸ªæ–‡æ¡£")
        return True
        
    except Exception as e:
        logger.error(f"âŒ éªŒè¯å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹ä¼˜åŒ–æ•°æ®åº“æ¢å¤...")
    
    # æ£€æŸ¥JSONæ–‡ä»¶
    if not os.path.exists("db_export.json"):
        logger.error("âŒ æœªæ‰¾åˆ° db_export.json æ–‡ä»¶")
        logger.info("è¯·ç¡®ä¿JSONå¯¼å‡ºæ–‡ä»¶å­˜åœ¨")
        return
    
    # å®Œå…¨æ¸…ç†å¹¶é‡æ–°åˆ›å»ºæ•°æ®åº“
    clean_and_recreate_db()
    
    # å¯¼å…¥JSONæ•°æ®
    if import_data_with_vectors():
        # éªŒè¯ç»“æœ
        if verify_database():
            logger.info("ğŸ‰ ä¼˜åŒ–æ•°æ®åº“æ¢å¤å®Œæˆï¼")
            logger.info("ğŸ’¡ æç¤ºï¼šå¦‚æœä½¿ç”¨äº†é¢„è®¡ç®—å‘é‡ï¼Œå·²é¿å…æ¨¡å‹ä¸‹è½½")
        else:
            logger.error("âŒ æ•°æ®åº“éªŒè¯å¤±è´¥")
    else:
        logger.error("âŒ æ•°æ®å¯¼å…¥å¤±è´¥")

if __name__ == "__main__":
    main() 