# 法院判例知识库项目文档

## 1. 项目目标

本项目旨在构建一个智能的法院判例知识库系统。该系统能够处理五种不同类型的法律案例（民事案例、刑事案例、行政案例、国家赔偿案例），并允许司法人员通过自然语言查询，快速找到相关的案例作为参考。
**新增目标：将检索到的案例内容与大语言模型（LLM）对接，以便对案例进行进一步的分析、总结或提取关键信息。**

## 2. 系统架构

系统主要包括数据处理、知识库构建、查询接口和**LLM分析**四个核心部分。

### 2.1 数据来源

*   五份包含不同类型案例的原始文档（最初为PDF，现已处理为 `.txt` 文件集合）。
    *   民事案例
    *   刑事案例
    *   行政案例
    *   执行案例
    *   国家赔偿案例

### 2.2 数据处理与准备流程

1.  **文本提取与初步整合**：
    *   （已完成）原始PDF内容被提取并整合为各大类下的 `all_cases.txt` 文件。
2.  **内容清理**：
    *   （已完成）针对 `all_cases.txt` 或已分割的案例文件，移除特定的元信息字符串和标准化的ID及页码串。
3.  **案例分割**：
    *   （已完成）将清理后的 `all_cases.txt` 文件按案件（以 `【` 为分隔符）分割成独立的 `.txt` 文件，并根据案件名规范化命名。这些文件存储在各自案例类型下的 `cases/`子目录中。

### 2.3 知识库构建

1.  **向量化模型**：
    *   （已完成）使用 `sentence-transformers` 库中的预训练模型 (`paraphrase-multilingual-MiniLM-L12-v2`) 将每个案例的文本内容转换为高维向量。
2.  **向量存储**：
    *   （已完成）使用 `ChromaDB` 作为向量数据库。
    *   （已完成）为五种案例类型分别创建独立的 ChromaDB **集合 (Collection)**，并进行了持久化存储。
    *   （已完成）解决了 `执行案例` 多案例文件拆分后的向量化更新问题。

### 2.4 查询接口

1.  **前端界面**：
    *   （已完成）构建了一个基于 `Flask` 和 HTML/CSS/JavaScript 的Web用户界面。
    *   （已完成）界面包含案例类型选择器、关键词输入框、发送按钮。
    *   （已完成）搜索结果以对话形式展示，用户查询居右，系统回复（案例）居左。
    *   （已完成）系统消息（案例）UI优化：文件名高亮、特定案情标题（如"基本案情"、"裁判理由"等）高亮、内容从"基本案情"开始展示、段落首行缩进、案例内容容器宽度调整。
    *   （已完成）为每个案例消息增加了右上角复制按钮，方便用户复制案例文本。
2.  **后端逻辑 (`app.py`)**：
    *   （已完成）Flask应用接收前端的查询请求和选定的案例类型。
    *   （已完成）根据选定的案例类型，连接到对应的ChromaDB集合。
    *   （已完成）将用户查询语句向量化。
    *   （已完成）在选定的ChromaDB集合中执行相似性搜索，检索最相关的案例。
    *   （已完成）将检索结果返回给前端展示。

### 2.5 LLM分析 (新增模块)
*   将从知识库检索到的案例内容发送给配置好的大语言模型。
*   LLM根据预设指令（或用户动态指令）对案例进行分析、总结、提取关键信息等。
*   将LLM的分析结果返回并在前端展示。

## 3. 核心组件与脚本

*   **`requirements.txt`**: （已更新）列出项目所需的Python依赖库（如 `sentence-transformers`, `chromadb`, `flask`, `pypinyin`）。
*   **`.gitignore`**: （已更新）指定Git版本控制系统应忽略的文件和目录（如 `venv/`, `__pycache__/`）。`db/chroma.sqlite3` 通过 Git LFS 追踪。
*   **`extract_cases.py`**: （已完成）从PDF提取文本，分割总文件为独立案例。
*   **`clean_single_all_cases_file.py`**: （已完成）清理大型文本文件。
*   **`clean_guiding_cases.py`**: （已完成）清理和重命名特定案例文件。
*   **`vectorize_and_store.py`**: （已完成）读取、向量化案例并存入ChromaDB。
*   **`app.py`**: （已完成基础功能）Flask Web应用，处理前端请求与ChromaDB交互。
*   **`templates/index.html`**: （已完成基础功能）前端页面。
*   **`PROJECT_DOCUMENTATION.md`**: (本文档)

## 4. 数据存储结构

```
.
├── 民事案例/
│   ├── all_cases.txt
│   └── cases/
├── 刑事案例/
│   ├── all_cases.txt
│   └── cases/
├── 行政案例/
│   ├── all_cases.txt
│   └── cases/
├── 执行案例/
│   ├── all_cases.txt
│   └── cases/
├── 国家赔偿案例/
│   ├── all_cases.txt
│   └── cases/
├── db/                       (ChromaDB 持久化数据存储目录, chroma.sqlite3通过Git LFS追踪)
│   └── chroma.sqlite3
│   └── ...
├── templates/
│   └── index.html
├── venv/
├── .gitattributes            (Git LFS 配置文件)
├── .gitignore
├── requirements.txt
├── extract_cases.py
├── clean_single_all_cases_file.py
├── clean_guiding_cases.py
├── vectorize_and_store.py
├── app.py
└── PROJECT_DOCUMENTATION.md
```
*注：`指导案/` 目录已确认废弃并移除。*

## 5. 工作流程概要

1.  **数据准备**：
    *   （已完成）原始PDF转换为各大类的 `all_cases.txt`。
    *   （已完成）使用清理脚本清理每个 `all_cases.txt` 及特定案例文件。
    *   （已完成）使用 `extract_cases.py` 将清理后的 `all_cases.txt` 分割为独立的案例文件到相应的 `cases/` 目录。
2.  **知识库填充**：
    *   （已完成）运行 `vectorize_and_store.py` 脚本，为所有案例类型的 `cases/` 目录下的文件进行处理，并存入各自的ChromaDB集合。
3.  **用户查询与初步结果展示**：
    *   （已完成）启动 `app.py` (Flask应用)。
    *   （已完成）用户通过浏览器访问Web界面，选择案例类型，输入查询。
    *   （已完成）系统从对应ChromaDB集合检索并返回相关案例列表，前端进行格式化展示。
4.  **LLM分析与结果展示 (后续步骤)**:
    *   在前端提供触发LLM分析的选项（例如，在每个案例下方增加一个"智能分析"按钮）。
    *   后端 `app.py` 增加新的API端点，接收案例内容和分析指令。
    *   后端调用LLM API，发送案例内容和指令。
    *   接收LLM返回的分析结果。
    *   在前端将LLM的分析结果以清晰的方式展示出来，可以是在原案例下方，或者一个新的消息区域。

## 6. 后续开发步骤

1.  **LLM集成设计与API选型**：
    *   确定用于案例分析的大语言模型（例如 OpenAI GPT系列, 或其他本地/云端模型）。
    *   研究LLM的API接口文档，了解调用方式、认证、速率限制等。
    *   设计合适的提示（Prompts）工程，以指导LLM进行有效的案例分析、摘要、关键信息提取等。
2.  **后端LLM接口实现 (`app.py`)**：
    *   增加新的Flask路由 (例如 `/analyze_case`) 用于接收前端发送的案例内容和可选的分析指令。
    *   实现调用选定LLM API的逻辑，包括错误处理和超时机制。
    *   处理LLM API的响应，提取分析结果。
3.  **前端LLM交互实现 (`templates/index.html`)**：
    *   在每个检索到的案例消息下方或旁边，添加一个"智能分析"或类似功能的按钮/链接。
    *   当用户点击该按钮时，通过JavaScript获取对应的案例内容。
    *   向后端新的 `/analyze_case` API端点发送异步请求（`fetch`）。
    *   在案例下方或新的消息区域优雅地展示LLM返回的分析结果，可能需要新的CSS样式和DOM操作。
    *   考虑增加加载状态提示，因为LLM分析可能需要一些时间。
4.  **测试与迭代**：
    *   对不同案例使用LLM分析功能，评估分析结果的质量和实用性。
    *   根据测试结果优化提示工程、调整LLM参数（如温度、最大token数）或选择不同的模型。
    *   收集用户反馈，持续改进LLM分析功能的交互和展示。
5.  **（可选）高级功能**：
    *   允许用户自定义分析指令或选择不同的分析模式（如"总结案情"、"提取争议焦点"、"分析裁判逻辑"等）。
    *   将LLM分析结果与原案例内容并排展示，方便对比。
    *   考虑对LLM的输出进行缓存，以减少重复请求和API调用成本。

## 7. 当前项目状态

*   数据预处理流程已完成，五大类案例数据已清理并分割为独立文件。
*   所有案例已成功向量化并存储在各自的ChromaDB集合中，数据库文件通过Git LFS管理。
*   Flask Web应用 (`app.py` 和 `templates/index.html`) 已搭建完成，具备核心的自然语言检索功能。
*   前端UI经过多轮迭代，实现了深色对话模式、底部输入区域优化、案例文件名和关键部分高亮、段落缩进、系统消息宽度调整及内容复制功能。
*   **第一阶段（基于向量的案例检索）已基本完成。项目进入第二阶段：集成大语言模型进行案例分析。** 